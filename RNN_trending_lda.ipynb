{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gensim.utils import simple_preprocess, tokenize\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "from gensim.parsing import preprocessing\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics # silhouette\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# pd.set_option('display.max_colwidth',1000)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/dev/cluster_12_cats_index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['title', 'body', 'subjects', 'date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_json('data/dev/cluster_12_cats_index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(labels, df, ids):\n",
    "    df2 = df.reset_index()\n",
    "    df2 = df2.set_index('id')\n",
    "\n",
    "    indexes = df2.loc[ids, 'index'].tolist()\n",
    "    \n",
    "    clusters = labels[indexes]\n",
    "    \n",
    "    return clusters\n",
    "    \n",
    "\n",
    "def visualize_trending(df):\n",
    "#     target = df[df['target'] == True]\n",
    "#     noise = df[df['target'] == False]\n",
    "#     t_group = target.groupby(['time'])['id'].count()\n",
    "#     n_group = noise.groupby(['category'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    # ax.plot(t_group.index, t_group.values, label='target')\n",
    "    \n",
    "    for name, group in df.groupby(['category']):\n",
    "        g = group.groupby(['time'])['id'].count()\n",
    "        ax.plot(g.index, g.values, label=name)\n",
    "    \n",
    "    ax.set(xlabel='time', ylabel='Numbers')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def get_target(df):\n",
    "    target = df[df['event'] != 'noise']['category'].iloc[0]\n",
    "    return target\n",
    "\n",
    "\n",
    "# def get_cluster_change(clusters, sample):\n",
    "#     changes = []\n",
    "    \n",
    "#     for i, ids in enumerate(clusters):\n",
    "#         # merge article index\n",
    "#         cluster = sample.iloc[ids]\n",
    "#         counts = cluster['time'].value_counts().sort_index()\n",
    "#         maj_class = cluster['category'].value_counts(normalize=True).index[0]\n",
    "#         # diff = times.diff().fillna(0)\n",
    "#         changes.append((i, maj_class, counts))\n",
    "        \n",
    "#     return changes\n",
    "\n",
    "def get_cluster_change(clusters, sample, target, timeline=100):\n",
    "    # sample['cluster'] = clusters\n",
    "    changes = []\n",
    "    \n",
    "    for i, ids in enumerate(clusters):\n",
    "        cluster = sample.iloc[ids]\n",
    "        # count and sort by timeline\n",
    "        counts = cluster['time'].value_counts().sort_index()\n",
    "        \n",
    "        if len(counts) < timeline:\n",
    "            miss_times = list(set(range(timeline)).difference(counts.index))\n",
    "            fill_values = pd.Series(np.zeros(len(miss_times)), index=miss_times)\n",
    "            counts = counts.append(fill_values)\n",
    "            counts = counts.sort_index()\n",
    "            \n",
    "        # print(cluster['time'].nunique())\n",
    "        class_count = cluster['category'].value_counts(normalize=True)\n",
    "        maj_class = class_count.index[0]\n",
    "        maj_percent = class_count[0]\n",
    "        # diff = times.diff().fillna(0)\n",
    "        # print(class_count[0])\n",
    "        is_target = (maj_class == target) and maj_percent > 0.5\n",
    "        \n",
    "        changes.append((counts.values, is_target, maj_class, maj_percent))\n",
    "        \n",
    "    return changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = kmean_cluster(doc_vecs, df, 2000, n_clusters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(models, open('data/dev/kmean_2k_models.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(\"data/dev/dataset_1_event.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_freq_change(samples, offset=0):\n",
    "    \n",
    "    all_changes = []\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(samples)):\n",
    "        #sample = pd.read_json('data/dev/samples/sample_' + str(i) + '.json')\n",
    "        sample.reset_index(inplace=True, drop=True)\n",
    "        #sample_vecs = get_doc_vecs(doc_vecs, df, sample['id'])\n",
    "        target = get_target(sample)\n",
    "        clusters = pickle.load(open(\"models/lda_cluster/clusters_\" + str(i + offset) + \".pkl\", \"rb\"))\n",
    "        freq_change = get_cluster_change(clusters, sample, target)\n",
    "        \n",
    "        all_changes += freq_change\n",
    "    \n",
    "    return all_changes\n",
    "\n",
    "\n",
    "\n",
    "def scale_x(X):\n",
    "    X = X + 1\n",
    "    return X / X[:, 0][:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9269562388156825\n",
      "0.9246871444823663\n",
      "0.9649122807017544\n",
      "0.7701983551040155\n",
      "0.3381881991664839\n",
      "0.8306962025316456\n",
      "0.7484472049689441\n",
      "0.8065043604651163\n",
      "0.3128491620111732\n",
      "0.7507712290956324\n",
      "0.8889545186060248\n",
      "0.818307426597582\n"
     ]
    }
   ],
   "source": [
    "target = get_target(sample)\n",
    "clusters = get_clusters(clustering.labels_, df, sample['id'])\n",
    "changes = get_cluster_change(clusters, sample, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = dataset[200:]\n",
    "test_samples = dataset[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [01:58<00:00, 15.18it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = get_all_freq_change(train_samples, offset=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:13<00:00, 15.19it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = get_all_freq_change(test_samples, offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 85,  82,  83,  79,  77,  86,  92,  76,  83,  99,  92,  83,  89,\n",
       "         90,  91,  83,  86,  92,  87,  85,  86,  79,  91,  82,  92,  76,\n",
       "         92,  84,  96,  83,  89,  81,  76,  79,  90,  94,  80,  72,  90,\n",
       "        100, 103,  88,  95,  72,  95,  85,  88,  82,  84,  92,  90,  87,\n",
       "         82,  82,  90,  94,  82,  94,  77,  75,  93,  89,  84,  90,  94,\n",
       "         85,  89,  76,  86,  91,  93,  88,  96,  86,  93,  98,  75,  86,\n",
       "         80,  74,  83,  77,  84,  88,  94,  92,  93,  85,  83,  92,  85,\n",
       "         92,  82,  97,  84,  97,  93, 110,  81,  96]),\n",
       " False,\n",
       " 'vaalit',\n",
       " 0.5980696311616683)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, c, _ = zip(*train_data)\n",
    "X_test, y_test, maj_cats, _ = zip(*test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(X)\n",
    "y = np.array(y)\n",
    "X_test = np.vstack(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=12)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_resampled = minmax_scale(X_resampled, axis=1)\n",
    "# X_test = minmax_scale(X_test, axis=1)\n",
    "X_resampled = scale_x(X_resampled)\n",
    "X_test = scale_x(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((65398, 100), (65398,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled.shape, y_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_change(sample, model, df):    \n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    target = get_target(sample)\n",
    "    clusters = get_clusters(model.labels_, df, sample['id'])\n",
    "    changes = get_cluster_change(clusters, sample, target)\n",
    "    index = np.arange(100)\n",
    "    distance = 0\n",
    "    \n",
    "    counts, is_target, cats, target_ratio = zip(*changes)\n",
    "    counts = np.array(counts) + 1\n",
    "    # print(counts.shape)\n",
    "    counts = counts / counts[:, 0][:,None]\n",
    "    # print(counts[-1])\n",
    "    \n",
    "    \n",
    "    for i, change in enumerate(is_target):\n",
    "        val = counts[i] + distance\n",
    "        \n",
    "        if change:\n",
    "            print(target_ratio[i])\n",
    "            ax.plot(index, val, ls='--')\n",
    "        else:\n",
    "            ax.plot(index, val)\n",
    "            \n",
    "        distance += 1\n",
    "    \n",
    "    ax.set(xlabel='Time', ylabel='Counts')\n",
    "    # ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1397\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clustering' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-30a47663ea8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvisualize_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclustering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clustering' is not defined"
     ]
    }
   ],
   "source": [
    "randid = np.random.randint(2000)\n",
    "sample = dataset[randid]\n",
    "print(randid)\n",
    "visualize_change(sample, clustering, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6540, 100)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.X[idx], self.y[idx]\n",
    "        \n",
    "        \n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FreqDataset(X_train, y_train)\n",
    "val_set = FreqDataset(X_val, y_val)\n",
    "test_set = FreqDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len=100, input_size=1, hidden_size=256, output_size=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.fc1 = nn.Linear(seq_len * hidden_size * 2, hidden_size)\n",
    "        # self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # self.hidden_cell = torch.zeros(2, 1, self.hidden_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        batch_size = input_seq.shape[0]\n",
    "        \n",
    "        lstm_out, (hid, cel) = self.lstm(input_seq.view(batch_size ,-1, self.input_size))\n",
    "#         print('lstm', lstm_out.shape)\n",
    "#         print('hid', hid[0].shape)\n",
    "        # out = self.fc1(hid.reshape(batch_size, -1))\n",
    "        out = self.fc1(lstm_out.reshape(batch_size, -1))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(dropout=0).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([10]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     31
    ]
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    # for i, batch in tqdm(enumerate(data_loader), total=len(data_loader), desc='Train'):\n",
    "    for batch in data_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "#         print('inputs', inputs.shape)\n",
    "#         print('output', outputs.shape)\n",
    "#         print('labels', labels.shape)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "#         print(outputs)\n",
    "#         print(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    accuracy = 0\n",
    "    y_pred = torch.tensor([], dtype=bool).to(device)\n",
    "    y_true = torch.tensor([], dtype=bool).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "#             print('output', outputs.shape)\n",
    "#             print('labels', labels.shape)\n",
    "            outputs = outputs.squeeze(1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            preds = (torch.sigmoid(outputs) >= 0.5)\n",
    "            y_pred = torch.cat([y_pred, preds])\n",
    "            y_true = torch.cat([y_true, labels])\n",
    "            \n",
    "#             accuracy += (preds == labels).sum()\n",
    "#             n_sample += len(labels)\n",
    "    \n",
    "    epoch_loss = epoch_loss / len(data_loader)\n",
    "    # accuracy = accuracy.cpu().item()\n",
    "    # accuracy = accuracy / n_sample\n",
    "    return epoch_loss, y_true.cpu().numpy(), y_pred.cpu().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Time: 0.18 mins | Train Loss: 0.097 | Val Loss: 0.108 | F1: 0.963\n",
      "Epoch: 2 | Time: 0.18 mins | Train Loss: 0.094 | Val Loss: 0.118 | F1: 0.951\n",
      "Epoch: 3 | Time: 0.17 mins | Train Loss: 0.091 | Val Loss: 0.111 | F1: 0.966\n",
      "Epoch: 4 | Time: 0.18 mins | Train Loss: 0.091 | Val Loss: 0.107 | F1: 0.965\n",
      "Epoch: 5 | Time: 0.18 mins | Train Loss: 0.086 | Val Loss: 0.115 | F1: 0.967\n",
      "Epoch: 6 | Time: 0.18 mins | Train Loss: 0.084 | Val Loss: 0.149 | F1: 0.939\n",
      "Epoch: 7 | Time: 0.18 mins | Train Loss: 0.080 | Val Loss: 0.111 | F1: 0.961\n",
      "Epoch: 8 | Time: 0.18 mins | Train Loss: 0.078 | Val Loss: 0.111 | F1: 0.963\n",
      "Epoch: 9 | Time: 0.18 mins | Train Loss: 0.078 | Val Loss: 0.104 | F1: 0.967\n",
      "Epoch: 10 | Time: 0.18 mins | Train Loss: 0.072 | Val Loss: 0.094 | F1: 0.971\n",
      "Epoch: 11 | Time: 0.18 mins | Train Loss: 0.070 | Val Loss: 0.110 | F1: 0.971\n",
      "Epoch: 12 | Time: 0.18 mins | Train Loss: 0.069 | Val Loss: 0.113 | F1: 0.971\n",
      "Epoch: 13 | Time: 0.18 mins | Train Loss: 0.066 | Val Loss: 0.098 | F1: 0.975\n",
      "Epoch: 14 | Time: 0.18 mins | Train Loss: 0.062 | Val Loss: 0.101 | F1: 0.971\n",
      "Epoch: 15 | Time: 0.18 mins | Train Loss: 0.062 | Val Loss: 0.100 | F1: 0.976\n",
      "Epoch: 16 | Time: 0.18 mins | Train Loss: 0.059 | Val Loss: 0.088 | F1: 0.978\n",
      "Epoch: 17 | Time: 0.18 mins | Train Loss: 0.056 | Val Loss: 0.118 | F1: 0.968\n",
      "Epoch: 18 | Time: 0.18 mins | Train Loss: 0.056 | Val Loss: 0.081 | F1: 0.977\n",
      "Epoch: 19 | Time: 0.18 mins | Train Loss: 0.052 | Val Loss: 0.103 | F1: 0.971\n",
      "Epoch: 20 | Time: 0.18 mins | Train Loss: 0.051 | Val Loss: 0.078 | F1: 0.981\n",
      "Epoch: 21 | Time: 0.18 mins | Train Loss: 0.053 | Val Loss: 0.085 | F1: 0.977\n",
      "Epoch: 22 | Time: 0.18 mins | Train Loss: 0.052 | Val Loss: 0.098 | F1: 0.974\n",
      "Epoch: 23 | Time: 0.18 mins | Train Loss: 0.050 | Val Loss: 0.079 | F1: 0.978\n",
      "Epoch: 24 | Time: 0.18 mins | Train Loss: 0.047 | Val Loss: 0.078 | F1: 0.980\n",
      "Epoch: 25 | Time: 0.18 mins | Train Loss: 0.044 | Val Loss: 0.087 | F1: 0.975\n",
      "Epoch: 26 | Time: 0.18 mins | Train Loss: 0.041 | Val Loss: 0.081 | F1: 0.980\n",
      "Epoch: 27 | Time: 0.18 mins | Train Loss: 0.045 | Val Loss: 0.064 | F1: 0.985\n",
      "Epoch: 28 | Time: 0.18 mins | Train Loss: 0.040 | Val Loss: 0.072 | F1: 0.981\n",
      "Epoch: 29 | Time: 0.18 mins | Train Loss: 0.040 | Val Loss: 0.076 | F1: 0.983\n",
      "Epoch: 30 | Time: 0.18 mins | Train Loss: 0.041 | Val Loss: 0.086 | F1: 0.983\n",
      "Epoch: 31 | Time: 0.18 mins | Train Loss: 0.036 | Val Loss: 0.085 | F1: 0.976\n",
      "Epoch: 32 | Time: 0.18 mins | Train Loss: 0.036 | Val Loss: 0.077 | F1: 0.983\n",
      "Epoch: 33 | Time: 0.18 mins | Train Loss: 0.038 | Val Loss: 0.106 | F1: 0.968\n",
      "Epoch: 34 | Time: 0.18 mins | Train Loss: 0.036 | Val Loss: 0.076 | F1: 0.982\n",
      "Epoch: 35 | Time: 0.18 mins | Train Loss: 0.035 | Val Loss: 0.068 | F1: 0.985\n",
      "Epoch: 36 | Time: 0.18 mins | Train Loss: 0.032 | Val Loss: 0.109 | F1: 0.979\n",
      "Epoch: 37 | Time: 0.18 mins | Train Loss: 0.037 | Val Loss: 0.069 | F1: 0.984\n",
      "Epoch: 38 | Time: 0.18 mins | Train Loss: 0.031 | Val Loss: 0.070 | F1: 0.985\n",
      "Epoch: 39 | Time: 0.18 mins | Train Loss: 0.033 | Val Loss: 0.060 | F1: 0.987\n",
      "Epoch: 40 | Time: 0.18 mins | Train Loss: 0.029 | Val Loss: 0.076 | F1: 0.986\n",
      "Epoch: 41 | Time: 0.18 mins | Train Loss: 0.029 | Val Loss: 0.066 | F1: 0.987\n",
      "Epoch: 42 | Time: 0.18 mins | Train Loss: 0.029 | Val Loss: 0.069 | F1: 0.986\n",
      "Epoch: 43 | Time: 0.18 mins | Train Loss: 0.031 | Val Loss: 0.072 | F1: 0.983\n",
      "Epoch: 44 | Time: 0.18 mins | Train Loss: 0.027 | Val Loss: 0.059 | F1: 0.985\n",
      "Epoch: 45 | Time: 0.18 mins | Train Loss: 0.027 | Val Loss: 0.072 | F1: 0.986\n",
      "Epoch: 46 | Time: 0.18 mins | Train Loss: 0.031 | Val Loss: 0.142 | F1: 0.958\n",
      "Epoch: 47 | Time: 0.18 mins | Train Loss: 0.027 | Val Loss: 0.074 | F1: 0.984\n",
      "Epoch: 48 | Time: 0.18 mins | Train Loss: 0.031 | Val Loss: 0.067 | F1: 0.987\n",
      "Epoch: 49 | Time: 0.18 mins | Train Loss: 0.019 | Val Loss: 0.071 | F1: 0.986\n",
      "Epoch: 50 | Time: 0.18 mins | Train Loss: 0.027 | Val Loss: 0.078 | F1: 0.986\n"
     ]
    }
   ],
   "source": [
    "N_EPOCH = 50\n",
    "for epoch in range(N_EPOCH):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, y_true, y_pred = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    duration = (time.time() - t0) / 60\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f'Epoch: {epoch+1} | Time: {duration:.2f} mins | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f} | F1: {f1:.3f}')\n",
    "    # print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/rnn/rnn_bi_lda.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, y_true, y_pred = evaluate(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, y_true, y_pred = evaluate(model, train_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.271284740883857, 0.21309925175072358)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3592,  133],\n",
       "       [  58,  217]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9841    0.9643    0.9741      3725\n",
      "         1.0     0.6200    0.7891    0.6944       275\n",
      "\n",
      "    accuracy                         0.9523      4000\n",
      "   macro avg     0.8021    0.8767    0.8343      4000\n",
      "weighted avg     0.9591    0.9523    0.9549      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_true == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>time</th>\n",
       "      <th>event</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3-6265210</td>\n",
       "      <td>autot</td>\n",
       "      <td>32</td>\n",
       "      <td>periodic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3-7781405</td>\n",
       "      <td>autot</td>\n",
       "      <td>34</td>\n",
       "      <td>periodic</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3-9431922</td>\n",
       "      <td>autot</td>\n",
       "      <td>89</td>\n",
       "      <td>periodic</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3-5058573</td>\n",
       "      <td>autot</td>\n",
       "      <td>44</td>\n",
       "      <td>periodic</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3-9155135</td>\n",
       "      <td>autot</td>\n",
       "      <td>98</td>\n",
       "      <td>periodic</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id category  time     event  cluster\n",
       "0  3-6265210    autot    32  periodic        1\n",
       "1  3-7781405    autot    34  periodic       19\n",
       "2  3-9431922    autot    89  periodic       14\n",
       "3  3-5058573    autot    44  periodic       19\n",
       "4  3-9155135    autot    98  periodic        3"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(df, clusters, y_pred, maj_cats, target):\n",
    "    accuracy = 0   # accuracy of detect correctly clusters have the target category as major\n",
    "    precision = 0  # accuracy of detect how many articles in cluster are target category\n",
    "    recall = 0    # accuracy of detect how many articles in target category are in choosen clusters\n",
    "    \n",
    "    pred_ids = np.argwhere(y_pred > 0).squeeze(1)\n",
    "    n_cluster = len(pred_ids)\n",
    "\n",
    "    df['target'] = (df['event'] != 'noise')\n",
    "    target_cluster = df[df['target'] == True]\n",
    "    \n",
    "    for i in pred_ids:\n",
    "        pred_cat = maj_cats[i]\n",
    "        cluster = df.iloc[clusters[i]]\n",
    "        \n",
    "        if pred_cat == target:\n",
    "            accuracy += 1\n",
    "        # calculate the prec and recall\n",
    "        if len(cluster['target']) > 0:\n",
    "            precision += cluster['target'].mean()\n",
    "        recall += cluster['target'].sum()\n",
    "    \n",
    "    # if there is output \n",
    "    if n_cluster:\n",
    "        accuracy = accuracy / n_cluster\n",
    "        precision = precision / n_cluster\n",
    "    \n",
    "    recall = recall / target_cluster.shape[0]\n",
    "    \n",
    "    return accuracy, precision, recall\n",
    "\n",
    "\n",
    "def evaluate_pipeline(samples, labels, preds, maj_cats, offset=0):\n",
    "    n_samples = len(samples)\n",
    "    acc = np.zeros(n_samples)\n",
    "    prc = np.zeros(n_samples)\n",
    "    rec = np.zeros(n_samples)\n",
    "    f = np.zeros(n_samples)\n",
    "    n_cluster = 20\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(samples)):\n",
    "        target = get_target(sample)\n",
    "        y_true = labels[i*n_cluster: (i+1)*n_cluster]\n",
    "        y_pred = preds[i*n_cluster: (i+1)*n_cluster]\n",
    "        cats = maj_cats[i*n_cluster: (i+1)*n_cluster]\n",
    "        \n",
    "        clusters = pickle.load(open(\"models/lda_cluster/clusters_\" + str(i + offset) + \".pkl\", \"rb\"))\n",
    "        \n",
    "        acc[i], prc[i], rec[i] = get_metrics(sample, clusters, y_pred, cats, target)\n",
    "        if (prc[i] + rec[i]) != 0:\n",
    "            f[i] = 2 * prc[i] * rec[i] / (prc[i] + rec[i])\n",
    "        \n",
    "    return acc, prc, rec, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 58.59it/s]\n"
     ]
    }
   ],
   "source": [
    "accs, precs, recs, f_micro = evaluate_pipeline(test_samples, y_true, y_pred, maj_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6818333333333334, 0.5236781561546279, 0.43449663001371763)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_macro =2 * precs.mean()*recs.mean()/(precs.mean()+recs.mean())\n",
    "accs.mean(), precs.mean(), recs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_micro: 0.4302670723083841 f_macro: 0.4749371353652097\n"
     ]
    }
   ],
   "source": [
    "f_micro = np.nan_to_num(f_micro)\n",
    "print(\"f_micro:\", f_micro.mean(), \"f_macro:\", f_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate stt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 'Talous',\n",
       " date\n",
       " 2007-01-01    250\n",
       " 2007-01-08    261\n",
       " 2007-01-15    325\n",
       " 2007-01-22    323\n",
       " 2007-01-29    368\n",
       "              ... \n",
       " 2008-10-27    345\n",
       " 2008-11-03    307\n",
       " 2008-11-10    313\n",
       " 2008-11-17    304\n",
       " 2008-11-24    292\n",
       " Freq: <pandas._libs.properties.CachedProperty object at 0x7ff1908a8460>, Length: 100, dtype: int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stt_changes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stt_changes():\n",
    "    changes = pickle.load(open(\"models/stt/stt_changes.pkl\", \"rb\"))\n",
    "    X = np.zeros((20, 100))\n",
    "    for i, s in enumerate(changes):\n",
    "        counts = s[2].values\n",
    "        X[i] = counts\n",
    "    \n",
    "    X = scale_x(X)\n",
    "    \n",
    "    return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stt_changes = get_stt_changes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.0438247 , 1.29880478, 1.29083665, 1.47011952,\n",
       "       1.34262948, 1.1314741 , 1.10756972, 1.60557769, 1.20318725,\n",
       "       1.63346614, 1.07171315, 1.36653386, 0.92031873, 1.15537849,\n",
       "       1.01992032, 1.27888446, 1.05976096, 1.03585657, 0.812749  ,\n",
       "       1.37450199, 1.12350598, 1.20717131, 1.25099602, 0.94422311,\n",
       "       0.99601594, 1.11553785, 1.25099602, 0.8685259 , 0.93625498,\n",
       "       1.36653386, 1.07569721, 1.33864542, 1.02390438, 1.03187251,\n",
       "       0.86055777, 0.92031873, 1.20318725, 1.13545817, 1.19920319,\n",
       "       1.2749004 , 1.08366534, 1.5059761 , 1.65737052, 0.9561753 ,\n",
       "       0.98007968, 1.11155378, 1.10756972, 0.94023904, 1.11553785,\n",
       "       1.59760956, 0.59760956, 0.89641434, 1.20318725, 1.27091633,\n",
       "       1.64940239, 1.38645418, 1.25896414, 1.18326693, 0.97609562,\n",
       "       1.1752988 , 1.27888446, 1.187251  , 0.90438247, 0.84462151,\n",
       "       0.85657371, 1.03984064, 0.9123506 , 1.05976096, 1.03585657,\n",
       "       1.03585657, 0.96414343, 1.02390438, 1.07569721, 0.97609562,\n",
       "       1.01593625, 1.05179283, 0.97609562, 1.05577689, 0.82071713,\n",
       "       1.01195219, 0.98804781, 1.1752988 , 0.78486056, 0.83266932,\n",
       "       0.84462151, 0.83665339, 1.        , 1.08366534, 1.21513944,\n",
       "       1.02390438, 1.23505976, 1.26294821, 1.3187251 , 1.38247012,\n",
       "       1.37848606, 1.22709163, 1.25099602, 1.21513944, 1.16733068])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stt_changes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, inputs):\n",
    "    model.eval()\n",
    "    y_pred = torch.tensor([], dtype=bool).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float).to(device)\n",
    "        print(inputs.shape)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        preds = (torch.sigmoid(outputs) >= 0.5)\n",
    "        y_pred = torch.cat([y_pred, preds])\n",
    "            \n",
    "    return y_pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 100])\n"
     ]
    }
   ],
   "source": [
    "stt_pred = infer(model, stt_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False,  True, False,\n",
       "       False,  True,  True, False, False,  True, False, False, False,\n",
       "        True, False])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stt_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
