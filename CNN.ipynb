{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "822e03a259ee26036b07c301b62192b42e51b9d09794aaca6c03133fdddfeb3f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gensim.utils import simple_preprocess, tokenize\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "from gensim.parsing import preprocessing\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics # silhouette\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import ruptures as rpt\n",
    "from ruptures.metrics import hausdorff, randindex\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# pd.set_option('display.max_colwidth',1000)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('data/prod/train_samples.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               freqs        pivots     pattern\n",
       "0  [0.8501266384547419, 0.9333112933106216, 0.968...      [11, 63]  spike_down\n",
       "1  [0.5642528003020821, 0.5783295540701987, 0.996...            []      stable\n",
       "2  [0.9017256704805636, 0.9229583934523402, 0.986...      [16, 82]        down\n",
       "3  [0.9460213693887373, 0.8285427047594923, 0.870...  [23, 57, 92]     down_up\n",
       "4  [1.0, 0.962602833443452, 0.9951430981134859, 0...      [19, 95]        down"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>freqs</th>\n      <th>pivots</th>\n      <th>pattern</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0.8501266384547419, 0.9333112933106216, 0.968...</td>\n      <td>[11, 63]</td>\n      <td>spike_down</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[0.5642528003020821, 0.5783295540701987, 0.996...</td>\n      <td>[]</td>\n      <td>stable</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.9017256704805636, 0.9229583934523402, 0.986...</td>\n      <td>[16, 82]</td>\n      <td>down</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0.9460213693887373, 0.8285427047594923, 0.870...</td>\n      <td>[23, 57, 92]</td>\n      <td>down_up</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[1.0, 0.962602833443452, 0.9951430981134859, 0...</td>\n      <td>[19, 95]</td>\n      <td>down</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pivots(df):\n",
    "    # convert pivots\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if row['pattern'] == 'up_down' or row['pattern'] == 'down_up':\n",
    "            row['pivots'] = np.array([row['pivots'][0], row['pivots'][2]])\n",
    "        elif row['pattern'] == 'spike_up' or row['pattern'] == 'spike_down':\n",
    "            temp = []\n",
    "            for p in row['pivots']:\n",
    "                temp.append(p-2)\n",
    "                temp.append(p+2)\n",
    "            row['pivots'] = np.array(temp)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this has been done in data_sampling file, no need to do this again\n",
    "df_train = convert_pivots(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(df_train['freqs'].values)\n",
    "y = df_train['pivots'].values\n",
    "z = df_train['pattern'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, z_train, z_val = train_test_split(X, y, z, test_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, pivots, events=None, timeline=100):\n",
    "        self.X = X\n",
    "        self.pivots = pivots\n",
    "        self.timeline = timeline\n",
    "        self.events = events\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.X[idx]\n",
    "        trg = torch.zeros(self.timeline, dtype=torch.float)\n",
    "        event = self.events[idx]\n",
    "        \n",
    "        if event == 'stable':\n",
    "            is_target = torch.tensor(0, dtype=torch.float)\n",
    "        else:\n",
    "            is_target = torch.tensor(1, dtype=torch.float)\n",
    "        \n",
    "            #trg = self.format_pivots(event, self.pivots[idx])\n",
    "            for i in self.pivots[idx]:\n",
    "                trg[i:] = trg[i] == 0\n",
    "            \n",
    "        #sos = torch.tensor([1])\n",
    "        #eos = torch.tensor([2])\n",
    "        \n",
    "        src = torch.tensor(src, dtype=torch.float)\n",
    "        # src = torch.cat((sos, src, eos))\n",
    "        # trg = torch.cat((sos, trg, eos))\n",
    "        # add sos and eos token id\n",
    "        # print(F.one_hot(trg))\n",
    "        # print(self.events[idx], self.pivots[idx])\n",
    "        return src, trg, is_target\n",
    "\n",
    "        \n",
    "    def format_pivots_2(self, event, pivots):\n",
    "        cur = 0\n",
    "        points = torch.zeros(self.timeline, dtype=torch.long)\n",
    "        for p in pivots:\n",
    "            if points[p] == cur:\n",
    "                points[p:] = cur + 1\n",
    "                cur += 1\n",
    "#             else:\n",
    "#                 points[p:] = 3\n",
    "                \n",
    "        return points\n",
    "    \n",
    "    def format_pivots(self, event, pivots):\n",
    "        # not using anywhere but just keep here if need\n",
    "        points = torch.zeros(self.timeline, dtype=torch.float)\n",
    "        \n",
    "        if event == 'spike_up':\n",
    "            for p in pivots:\n",
    "                points[p-2: p] = 1\n",
    "                points[p: p+3] = 1\n",
    "                \n",
    "        elif event == 'spike_down':\n",
    "            for p in pivots:\n",
    "                points[p-2: p] = 1\n",
    "                points[p: p+3] = 1\n",
    "                \n",
    "        elif event == 'up_down':\n",
    "            points[pivots[0]: pivots[1]] = 1\n",
    "            #points[pivots[1]: pivots[2]] = 1\n",
    "            \n",
    "        elif event == 'down_up':\n",
    "            points[pivots[0]: pivots[1]] = 1\n",
    "            #points[pivots[1]: pivots[2]] = 1\n",
    "            \n",
    "        elif event == 'up':\n",
    "            points[pivots[0]: pivots[1]] = 1\n",
    "            \n",
    "        elif event == 'down':\n",
    "            points[pivots[0]: pivots[1]] = 1\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        return points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SeqDataset(X_train, y_train, z_train)\n",
    "val_set = SeqDataset(X_val, y_val, z_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hid_dim, out_dim, cov_dim=8, dropout=0.5, device=device):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.cov_dim = cov_dim\n",
    "        self.device = device\n",
    "        self.embed = torch.nn.Embedding(out_dim, hid_dim)\n",
    "        seq_len = 100\n",
    "        \n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=0),  # (100 - 3) + 1 = 98\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(2, 2),  # (98 - 2) / 2 + 1 = 49\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=0), # (49 - 3) + 1 = 47\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(3, 2), # (47 - 3) / 2 + 1 = 23\n",
    "        )\n",
    "        \n",
    "        # use both RNN encoded and CNN output for decoder\n",
    "        \n",
    "        self.seq_out = nn.Linear(hid_dim, out_dim)\n",
    "        self.fc = nn.Linear(23 * 16, hid_dim)\n",
    "        \n",
    "        self.bin_out = nn.Linear(hid_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, src):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = src.shape[1]\n",
    "        \n",
    "        conv_out = self.conv(src.unsqueeze(1))\n",
    "        \n",
    "        cnn_out = conv_out.view(batch_size, -1)\n",
    "        context = self.dropout(cnn_out)\n",
    "        \n",
    "        fc_out = self.fc(context)\n",
    "        \n",
    "        seq_out = self.seq_out(F.relu(fc_out))\n",
    "        bin_out = self.bin_out(F.relu(fc_out))\n",
    "        \n",
    "        bin_out = torch.sigmoid(bin_out)\n",
    "        seq_out = torch.sigmoid(seq_out)\n",
    "        \n",
    "        return seq_out, bin_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'models/rnn_cnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(y_bin, y_seq):\n",
    "    founds = []\n",
    "    # founds = np.where(y_bin > 0)[0]\n",
    "    \n",
    "    for i, (x, p) in enumerate(zip(y_bin, y_seq)):\n",
    "        pivots = get_pivots(p)\n",
    "        if len(pivots) and x:\n",
    "            founds.append(i)\n",
    "    return founds\n",
    "\n",
    "\n",
    "def get_pivots(pred):\n",
    "    # convert sequence of 0 and 1 to list of pivots\n",
    "    pivots = []\n",
    "\n",
    "    cur = pred[0]\n",
    "    for i, v in enumerate(pred):\n",
    "#         if v == 1 or v == 2:\n",
    "#             # print(v)\n",
    "#             continue\n",
    "        if v != cur:\n",
    "            pivots.append(i)\n",
    "            cur = v\n",
    "    return pivots\n",
    "\n",
    "def visualize_train_data(X, y, y_true=[]):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    index = np.arange(100)\n",
    "    \n",
    "    ax.plot(index, X)\n",
    "    ax.vlines(y, ymin=0, ymax=1, color='red')\n",
    "    if len(y_true):\n",
    "        ax.vlines(y_true + 0.5, ymin=0, ymax=1, color='green')\n",
    "    \n",
    "    ax.set(xlabel='Time', ylabel='Counts')\n",
    "    # ax.legend()\n",
    "    ax.set_ylim(0)\n",
    "    ax.grid()\n",
    "    ax.set_xticks(range(0, 100, 2))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "def get_report(y_bin, y_seq, label_points, timeline=100):\n",
    "    true_bin = np.array([bool(len(p)) for p in label_points])\n",
    "\n",
    "    print(classification_report(y_bin, true_bin, digits=4))\n",
    "    randidx_pos = 0\n",
    "    randidx_neg = 0\n",
    "    count_neg = 0\n",
    "    count_pos = 0\n",
    "    \n",
    "    for i, true_points in enumerate(label_points):\n",
    "        true_points = np.concatenate((true_points, [timeline]), axis=0).astype(int)\n",
    "        \n",
    "        pred_points = get_pivots(y_seq[i])\n",
    "        # print(pred_points)\n",
    "        if timeline not in pred_points:\n",
    "            pred_points = np.concatenate((pred_points, [timeline]), axis=0).astype(int)\n",
    "            \n",
    "        if len(pred_points) > 1:\n",
    "            count_pos += 1\n",
    "            randidx_pos += randindex(true_points, pred_points)\n",
    "        else:\n",
    "            count_neg += 1\n",
    "            randidx_neg += randindex(true_points, pred_points)\n",
    "        # print(t, p)\n",
    "        # print(true_points)\n",
    "        \n",
    "    #print(len(true_bin), true_bin.sum(), count, randidx_neg, randidx_pos)\n",
    "    print(\"Rand Index Positive:\", randidx_pos/count_pos)\n",
    "    print(\"Rand Index Negative:\", randidx_neg/count_neg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, y_bin, y_seq = test(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_report(y_bin, y_seq, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_result(y_bin, y_seq)\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = np.random.choice(result)\n",
    "print(test_id)\n",
    "print(get_pivots(y_seq[test_id]))\n",
    "print(y_val[test_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_train_data(X_val[test_id], get_pivots(y_seq[test_id]), y_val[test_id])"
   ]
  }
 ]
}